{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim\n",
    "\n",
    "(This tutorial is based on [This](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W84fpi-ZNTY))\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making an assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec. The performance of word embeddings depends on two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To get started, you will need to install Gensim. The first cell below does that; ignore it if you already have gensim installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "Collecting gensim\n",
      "\u001b[33m  WARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 5.5 MB/s \n",
      "\u001b[?25h\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "Collecting smart-open>=1.8.1\n",
      "\u001b[33m  WARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 41.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/leander/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/leander/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/leander/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in /home/leander/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/leander/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/leander/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leander/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/leander/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=9b66b9b8b93449020b138cc44bfd698e2fad7ca8137f67aa8fdd797cb691e5e2\n",
      "  Stored in directory: /home/leander/.cache/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use IMDB Movie Review Dataset [Download](http://ai.stanford.edu/~amaas/data/sentiment/) (feel free to use other datasets). This dataset has full user reviews of movies (50,000 positive + negative reviews for training). I concatenated all the positive and negative reviews in one text file, which is available for download in Blackboard. \n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b\"once again mr .  costner has dragged out a movie for far longer than necessary .  aside from the terrific sea rescue sequences ,  of which there are very few i just did not care about any of the characters .  most of us have ghosts in the closet ,  and costner's character are realized early on ,  and then forgotten until much later ,  by which time i did not care .  the character we should really care about is a very cocky ,  overconfident ashton kutcher .  the problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet .  his only obstacle appears to be winning over costner .  finally when we are well past the half way point of this stinker ,  costner tells us all about kutcher's ghosts .  we are told why kutcher is driven to be the best with no prior inkling or foreshadowing .  no magic here ,  it was all i could do to keep from turning it off an hour in . \\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file=\"imdb.txt.gz\"\n",
    "\n",
    "with gzip.open (data_file, 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-15 13:51:36,135 : INFO : reading file imdb.txt.gz...this may take a while\n",
      "2020-11-15 13:51:36,137 : INFO : read 0 reviews\n",
      "2020-11-15 13:51:38,687 : INFO : read 10000 reviews\n",
      "2020-11-15 13:51:41,324 : INFO : read 20000 reviews\n",
      "2020-11-15 13:51:43,864 : INFO : read 30000 reviews\n",
      "2020-11-15 13:51:46,529 : INFO : read 40000 reviews\n",
      "2020-11-15 13:51:49,200 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the IMDB dataset takes about 5 minutes or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ole',\n",
       "  'is',\n",
       "  'seen',\n",
       "  'coming',\n",
       "  'out',\n",
       "  'from',\n",
       "  'behind',\n",
       "  'some',\n",
       "  'boxes',\n",
       "  'in',\n",
       "  'the',\n",
       "  'closet',\n",
       "  'she',\n",
       "  'would',\n",
       "  'have',\n",
       "  'been',\n",
       "  'easily',\n",
       "  'spotted',\n",
       "  'if',\n",
       "  'the',\n",
       "  'cop',\n",
       "  'had',\n",
       "  'spent',\n",
       "  'all',\n",
       "  'of',\n",
       "  'seconds',\n",
       "  'looking',\n",
       "  'apparently',\n",
       "  'too',\n",
       "  'stupid',\n",
       "  'to',\n",
       "  'have',\n",
       "  'said',\n",
       "  'or',\n",
       "  'done',\n",
       "  'anything',\n",
       "  'when',\n",
       "  'the',\n",
       "  'policeman',\n",
       "  'was',\n",
       "  'there',\n",
       "  'wow',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'apparently',\n",
       "  'the',\n",
       "  'first',\n",
       "  'in',\n",
       "  'new',\n",
       "  'line',\n",
       "  'of',\n",
       "  'quality',\n",
       "  'direct',\n",
       "  'to',\n",
       "  'dvd',\n",
       "  'movies',\n",
       "  'marketed',\n",
       "  'as',\n",
       "  'being',\n",
       "  'too',\n",
       "  'extreme',\n",
       "  'for',\n",
       "  'theaters',\n",
       "  'in',\n",
       "  'reality',\n",
       "  'it',\n",
       "  'just',\n",
       "  'more',\n",
       "  'cliché',\n",
       "  'movie',\n",
       "  'garbage'],\n",
       " ['this',\n",
       "  'movie',\n",
       "  'has',\n",
       "  'got',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'disappointment',\n",
       "  've',\n",
       "  'ever',\n",
       "  'experienced',\n",
       "  'with',\n",
       "  'film',\n",
       "  'the',\n",
       "  'acting',\n",
       "  'is',\n",
       "  'horrific',\n",
       "  'the',\n",
       "  'suspense',\n",
       "  'build',\n",
       "  'up',\n",
       "  'minimal',\n",
       "  'and',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'overall',\n",
       "  'is',\n",
       "  'ridiculous',\n",
       "  'found',\n",
       "  'myself',\n",
       "  'rooting',\n",
       "  'for',\n",
       "  'the',\n",
       "  'victim',\n",
       "  'to',\n",
       "  'just',\n",
       "  'hurry',\n",
       "  'up',\n",
       "  'and',\n",
       "  'become',\n",
       "  'victim',\n",
       "  'because',\n",
       "  'she',\n",
       "  'obviously',\n",
       "  'needed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'put',\n",
       "  'out',\n",
       "  'of',\n",
       "  'her',\n",
       "  'misery',\n",
       "  'anyone',\n",
       "  'with',\n",
       "  'rudimentary',\n",
       "  'knowledge',\n",
       "  'of',\n",
       "  'how',\n",
       "  'the',\n",
       "  'world',\n",
       "  'works',\n",
       "  'will',\n",
       "  'immediately',\n",
       "  'be',\n",
       "  'disgusted',\n",
       "  'at',\n",
       "  'the',\n",
       "  'leaps',\n",
       "  'we',\n",
       "  're',\n",
       "  'asked',\n",
       "  'to',\n",
       "  'make',\n",
       "  'in',\n",
       "  'logic',\n",
       "  'and',\n",
       "  'the',\n",
       "  'so',\n",
       "  'called',\n",
       "  'suspenseful',\n",
       "  'buildup',\n",
       "  'would',\n",
       "  'be',\n",
       "  'lucky',\n",
       "  'to',\n",
       "  'get',\n",
       "  'year',\n",
       "  'old',\n",
       "  'to',\n",
       "  'be',\n",
       "  'mildly',\n",
       "  'worried',\n",
       "  'dismayed',\n",
       "  'that',\n",
       "  'sequel',\n",
       "  'is',\n",
       "  'planned',\n",
       "  'because',\n",
       "  'it',\n",
       "  'means',\n",
       "  'they',\n",
       "  'll',\n",
       "  'be',\n",
       "  'asking',\n",
       "  'us',\n",
       "  'to',\n",
       "  'once',\n",
       "  'again',\n",
       "  'swallow',\n",
       "  'sub',\n",
       "  'par',\n",
       "  'plot',\n",
       "  'line',\n",
       "  'if',\n",
       "  'this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'example',\n",
       "  'of',\n",
       "  'raw',\n",
       "  'feed',\n",
       "  'work',\n",
       "  'think',\n",
       "  'll',\n",
       "  'be',\n",
       "  'avoiding',\n",
       "  'any',\n",
       "  'and',\n",
       "  'all',\n",
       "  'future',\n",
       "  'films',\n",
       "  'by',\n",
       "  'them'],\n",
       " ['there',\n",
       "  'something',\n",
       "  'going',\n",
       "  'on',\n",
       "  'in',\n",
       "  'this',\n",
       "  'film',\n",
       "  'directed',\n",
       "  'by',\n",
       "  'files',\n",
       "  'scribe',\n",
       "  'john',\n",
       "  'shiban',\n",
       "  'that',\n",
       "  'has',\n",
       "  'eluded',\n",
       "  'me',\n",
       "  'you',\n",
       "  'get',\n",
       "  'that',\n",
       "  'feeling',\n",
       "  'as',\n",
       "  'the',\n",
       "  'film',\n",
       "  'moves',\n",
       "  'that',\n",
       "  'everything',\n",
       "  'is',\n",
       "  'not',\n",
       "  'what',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'yet',\n",
       "  'feel',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'fails',\n",
       "  'at',\n",
       "  'giving',\n",
       "  'you',\n",
       "  'enough',\n",
       "  'to',\n",
       "  'go',\n",
       "  'on',\n",
       "  'to',\n",
       "  'truly',\n",
       "  'care',\n",
       "  'afterward',\n",
       "  'it',\n",
       "  'about',\n",
       "  'perception',\n",
       "  'there',\n",
       "  'are',\n",
       "  'characters',\n",
       "  'the',\n",
       "  'heroine',\n",
       "  'nicole',\n",
       "  'jaimie',\n",
       "  'alexander',\n",
       "  'meets',\n",
       "  'in',\n",
       "  'the',\n",
       "  'film',\n",
       "  'that',\n",
       "  'she',\n",
       "  'talks',\n",
       "  'to',\n",
       "  'that',\n",
       "  'up',\n",
       "  'and',\n",
       "  'vanish',\n",
       "  'this',\n",
       "  'might',\n",
       "  'seem',\n",
       "  'like',\n",
       "  'spoiler',\n",
       "  'but',\n",
       "  'it',\n",
       "  'something',\n",
       "  'that',\n",
       "  'really',\n",
       "  'only',\n",
       "  'inherits',\n",
       "  'wee',\n",
       "  'bit',\n",
       "  'of',\n",
       "  'focus',\n",
       "  'on',\n",
       "  'the',\n",
       "  'filmmakers',\n",
       "  'part',\n",
       "  'they',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'be',\n",
       "  'poking',\n",
       "  'fun',\n",
       "  'at',\n",
       "  'us',\n",
       "  'as',\n",
       "  'we',\n",
       "  'watch',\n",
       "  'curious',\n",
       "  'at',\n",
       "  'whether',\n",
       "  'we',\n",
       "  'should',\n",
       "  'trust',\n",
       "  'what',\n",
       "  'nicole',\n",
       "  'is',\n",
       "  'seeing',\n",
       "  'or',\n",
       "  'not',\n",
       "  'it',\n",
       "  'never',\n",
       "  'gets',\n",
       "  'proper',\n",
       "  'answer',\n",
       "  'and',\n",
       "  'for',\n",
       "  'one',\n",
       "  'was',\n",
       "  'bit',\n",
       "  'clueless',\n",
       "  'at',\n",
       "  'the',\n",
       "  'point',\n",
       "  'there',\n",
       "  'comes',\n",
       "  'time',\n",
       "  'in',\n",
       "  'film',\n",
       "  'when',\n",
       "  'ambiguity',\n",
       "  'can',\n",
       "  'just',\n",
       "  'be',\n",
       "  'frustrating',\n",
       "  'because',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  'is',\n",
       "  'led',\n",
       "  'on',\n",
       "  'wild',\n",
       "  'goose',\n",
       "  'chase',\n",
       "  'that',\n",
       "  'ends',\n",
       "  'at',\n",
       "  'dead',\n",
       "  'end',\n",
       "  'with',\n",
       "  'little',\n",
       "  'explanation',\n",
       "  'at',\n",
       "  'what',\n",
       "  'we',\n",
       "  'just',\n",
       "  'saw',\n",
       "  'it',\n",
       "  'ultimately',\n",
       "  'feels',\n",
       "  'like',\n",
       "  'an',\n",
       "  'exhausting',\n",
       "  'exercise',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'thrilling',\n",
       "  'psycho',\n",
       "  'drama',\n",
       "  'now',\n",
       "  'there',\n",
       "  'nothing',\n",
       "  'wrong',\n",
       "  'with',\n",
       "  'ambiguity',\n",
       "  'itself',\n",
       "  'but',\n",
       "  'give',\n",
       "  'us',\n",
       "  'something',\n",
       "  'to',\n",
       "  'latch',\n",
       "  'onto',\n",
       "  'or',\n",
       "  'you',\n",
       "  'will',\n",
       "  'evade',\n",
       "  'us',\n",
       "  'that',\n",
       "  'how',\n",
       "  'felt',\n",
       "  'as',\n",
       "  'watched',\n",
       "  'rest',\n",
       "  'stop',\n",
       "  'this',\n",
       "  'film',\n",
       "  'is',\n",
       "  'supposedly',\n",
       "  'about',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'named',\n",
       "  'nicole',\n",
       "  'who',\n",
       "  'decides',\n",
       "  'to',\n",
       "  'run',\n",
       "  'off',\n",
       "  'to',\n",
       "  'california',\n",
       "  'with',\n",
       "  'lover',\n",
       "  'jesse',\n",
       "  'joey',\n",
       "  'mendicino',\n",
       "  'to',\n",
       "  'make',\n",
       "  'it',\n",
       "  'big',\n",
       "  'in',\n",
       "  'hollywood',\n",
       "  'they',\n",
       "  'make',\n",
       "  'what',\n",
       "  'is',\n",
       "  'supposed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'slight',\n",
       "  'detour',\n",
       "  'at',\n",
       "  'rest',\n",
       "  'stop',\n",
       "  'so',\n",
       "  'that',\n",
       "  'nicole',\n",
       "  'can',\n",
       "  'pee',\n",
       "  'but',\n",
       "  'it',\n",
       "  'descends',\n",
       "  'into',\n",
       "  'terror',\n",
       "  'for',\n",
       "  'her',\n",
       "  'when',\n",
       "  'she',\n",
       "  'finds',\n",
       "  'that',\n",
       "  'her',\n",
       "  'jesse',\n",
       "  'is',\n",
       "  'completely',\n",
       "  'missing',\n",
       "  'someone',\n",
       "  'in',\n",
       "  'crusty',\n",
       "  'dusty',\n",
       "  'yellow',\n",
       "  'truck',\n",
       "  'is',\n",
       "  'nut',\n",
       "  'job',\n",
       "  'who',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'be',\n",
       "  'causing',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'trouble',\n",
       "  'to',\n",
       "  'nicole',\n",
       "  'and',\n",
       "  'we',\n",
       "  'soon',\n",
       "  'realize',\n",
       "  'that',\n",
       "  'he',\n",
       "  'is',\n",
       "  'behind',\n",
       "  'jesse',\n",
       "  'disappearance',\n",
       "  'when',\n",
       "  'things',\n",
       "  'start',\n",
       "  'to',\n",
       "  'occur',\n",
       "  'signs',\n",
       "  'provided',\n",
       "  'to',\n",
       "  'her',\n",
       "  'if',\n",
       "  'you',\n",
       "  'will',\n",
       "  'she',\n",
       "  'will',\n",
       "  'have',\n",
       "  'to',\n",
       "  'find',\n",
       "  'way',\n",
       "  'out',\n",
       "  'of',\n",
       "  'very',\n",
       "  'difficult',\n",
       "  'situation',\n",
       "  'nicole',\n",
       "  'is',\n",
       "  'far',\n",
       "  'from',\n",
       "  'any',\n",
       "  'existing',\n",
       "  'town',\n",
       "  'and',\n",
       "  'with',\n",
       "  'limited',\n",
       "  'resources',\n",
       "  'to',\n",
       "  'defend',\n",
       "  'herself',\n",
       "  'against',\n",
       "  'maniac',\n",
       "  'who',\n",
       "  'provides',\n",
       "  'her',\n",
       "  'with',\n",
       "  'some',\n",
       "  'strong',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'how',\n",
       "  'evil',\n",
       "  'he',\n",
       "  'can',\n",
       "  'be',\n",
       "  'that',\n",
       "  'the',\n",
       "  'easy',\n",
       "  'part',\n",
       "  'when',\n",
       "  'female',\n",
       "  'character',\n",
       "  'comes',\n",
       "  'into',\n",
       "  'play',\n",
       "  'the',\n",
       "  'film',\n",
       "  'makes',\n",
       "  'really',\n",
       "  'bizarre',\n",
       "  'leap',\n",
       "  'from',\n",
       "  'logic',\n",
       "  'as',\n",
       "  'we',\n",
       "  'are',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'where',\n",
       "  'she',\n",
       "  'ever',\n",
       "  'came',\n",
       "  'from',\n",
       "  'how',\n",
       "  'she',\n",
       "  'got',\n",
       "  'there',\n",
       "  'and',\n",
       "  'more',\n",
       "  'importantly',\n",
       "  'where',\n",
       "  'she',\n",
       "  'goes',\n",
       "  'once',\n",
       "  'nicole',\n",
       "  'tries',\n",
       "  'to',\n",
       "  'break',\n",
       "  'her',\n",
       "  'free',\n",
       "  'from',\n",
       "  'her',\n",
       "  'supposed',\n",
       "  'prison',\n",
       "  'in',\n",
       "  'the',\n",
       "  'restroom',\n",
       "  'utilities',\n",
       "  'cabinet',\n",
       "  'she',\n",
       "  'meets',\n",
       "  'another',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'in',\n",
       "  'the',\n",
       "  'area',\n",
       "  'joey',\n",
       "  'lawrence',\n",
       "  'who',\n",
       "  'might',\n",
       "  'seem',\n",
       "  'like',\n",
       "  'her',\n",
       "  'savior',\n",
       "  'but',\n",
       "  'when',\n",
       "  'he',\n",
       "  'too',\n",
       "  'is',\n",
       "  'victim',\n",
       "  'of',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'driver',\n",
       "  'startling',\n",
       "  'things',\n",
       "  'occur',\n",
       "  'again',\n",
       "  'that',\n",
       "  'questions',\n",
       "  'if',\n",
       "  'he',\n",
       "  'was',\n",
       "  'ever',\n",
       "  'actually',\n",
       "  'even',\n",
       "  'there',\n",
       "  'to',\n",
       "  'begin',\n",
       "  'with',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'driver',\n",
       "  'commits',\n",
       "  'torturous',\n",
       "  'acts',\n",
       "  'to',\n",
       "  'nicole',\n",
       "  'like',\n",
       "  'holing',\n",
       "  'her',\n",
       "  'up',\n",
       "  'in',\n",
       "  'the',\n",
       "  'restroom',\n",
       "  'and',\n",
       "  'as',\n",
       "  'she',\n",
       "  'tries',\n",
       "  'to',\n",
       "  'untie',\n",
       "  'wire',\n",
       "  'that',\n",
       "  'the',\n",
       "  'killer',\n",
       "  'has',\n",
       "  'wrapped',\n",
       "  'around',\n",
       "  'the',\n",
       "  'door',\n",
       "  'lock',\n",
       "  'she',\n",
       "  'receives',\n",
       "  'nasty',\n",
       "  'bite',\n",
       "  'from',\n",
       "  'him',\n",
       "  'he',\n",
       "  'then',\n",
       "  'sets',\n",
       "  'fire',\n",
       "  'to',\n",
       "  'the',\n",
       "  'restroom',\n",
       "  'leaving',\n",
       "  'her',\n",
       "  'without',\n",
       "  'lasting',\n",
       "  'place',\n",
       "  'of',\n",
       "  'refuge',\n",
       "  'from',\n",
       "  'the',\n",
       "  'beast',\n",
       "  'it',\n",
       "  'the',\n",
       "  'timing',\n",
       "  'of',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'driver',\n",
       "  'attacks',\n",
       "  'that',\n",
       "  'has',\n",
       "  'me',\n",
       "  'listless',\n",
       "  'perhaps',\n",
       "  'he',\n",
       "  'just',\n",
       "  'likes',\n",
       "  'tormenting',\n",
       "  'her',\n",
       "  'but',\n",
       "  'he',\n",
       "  'appropriately',\n",
       "  'appears',\n",
       "  'in',\n",
       "  'certain',\n",
       "  'situations',\n",
       "  'where',\n",
       "  'nicole',\n",
       "  'has',\n",
       "  'time',\n",
       "  'to',\n",
       "  'flee',\n",
       "  'or',\n",
       "  'prepare',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  'make',\n",
       "  'much',\n",
       "  'sense',\n",
       "  'his',\n",
       "  'motives',\n",
       "  'which',\n",
       "  'propel',\n",
       "  'the',\n",
       "  'film',\n",
       "  'into',\n",
       "  'an',\n",
       "  'illogical',\n",
       "  'idea',\n",
       "  'why',\n",
       "  'does',\n",
       "  'he',\n",
       "  'make',\n",
       "  'himself',\n",
       "  'so',\n",
       "  'obvious',\n",
       "  'why',\n",
       "  'does',\n",
       "  'he',\n",
       "  'allow',\n",
       "  'her',\n",
       "  'to',\n",
       "  'prepare',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'going',\n",
       "  'out',\n",
       "  'on',\n",
       "  'limb',\n",
       "  'here',\n",
       "  'that',\n",
       "  'he',\n",
       "  'likes',\n",
       "  'having',\n",
       "  'his',\n",
       "  'quarry',\n",
       "  'believe',\n",
       "  'they',\n",
       "  'can',\n",
       "  'find',\n",
       "  'way',\n",
       "  'of',\n",
       "  'escape',\n",
       "  'only',\n",
       "  'to',\n",
       "  'stomp',\n",
       "  'that',\n",
       "  'hope',\n",
       "  'out',\n",
       "  'when',\n",
       "  'he',\n",
       "  'comes',\n",
       "  'up',\n",
       "  'with',\n",
       "  'his',\n",
       "  'next',\n",
       "  'grisly',\n",
       "  'attack',\n",
       "  'yet',\n",
       "  'why',\n",
       "  'does',\n",
       "  'shiban',\n",
       "  'decide',\n",
       "  'to',\n",
       "  'play',\n",
       "  'with',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  'by',\n",
       "  'having',\n",
       "  'nicole',\n",
       "  'experience',\n",
       "  'odd',\n",
       "  'meetings',\n",
       "  'with',\n",
       "  'people',\n",
       "  'that',\n",
       "  'don',\n",
       "  'exist',\n",
       "  'what',\n",
       "  'is',\n",
       "  'shiban',\n",
       "  'and',\n",
       "  'the',\n",
       "  'writing',\n",
       "  'team',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'say',\n",
       "  'and',\n",
       "  'to',\n",
       "  'cap',\n",
       "  'off',\n",
       "  'the',\n",
       "  'film',\n",
       "  'unhinged',\n",
       "  'weirdness',\n",
       "  'is',\n",
       "  'family',\n",
       "  'in',\n",
       "  'rv',\n",
       "  'they',\n",
       "  'play',\n",
       "  'small',\n",
       "  'part',\n",
       "  'in',\n",
       "  'the',\n",
       "  'scheme',\n",
       "  'of',\n",
       "  'things',\n",
       "  'as',\n",
       "  'religious',\n",
       "  'bigots',\n",
       "  'themselves',\n",
       "  'but',\n",
       "  'the',\n",
       "  'film',\n",
       "  'doesn',\n",
       "  'do',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  'to',\n",
       "  'explain',\n",
       "  'why',\n",
       "  'they',\n",
       "  'should',\n",
       "  'be',\n",
       "  'in',\n",
       "  'this',\n",
       "  'film',\n",
       "  'at',\n",
       "  'all'],\n",
       " ['young',\n",
       "  'woman',\n",
       "  'nicole',\n",
       "  'carrow',\n",
       "  'jaimie',\n",
       "  'alexander',\n",
       "  'and',\n",
       "  'her',\n",
       "  'boyfriend',\n",
       "  'jess',\n",
       "  'joey',\n",
       "  'mendicino',\n",
       "  'become',\n",
       "  'targets',\n",
       "  'for',\n",
       "  'deranged',\n",
       "  'serial',\n",
       "  'killer',\n",
       "  'after',\n",
       "  'stopping',\n",
       "  'for',\n",
       "  'comfort',\n",
       "  'break',\n",
       "  'at',\n",
       "  'remote',\n",
       "  'road',\n",
       "  'side',\n",
       "  'rest',\n",
       "  'stop',\n",
       "  'what',\n",
       "  'might',\n",
       "  'have',\n",
       "  'been',\n",
       "  'an',\n",
       "  'effectively',\n",
       "  'scary',\n",
       "  'chiller',\n",
       "  'in',\n",
       "  'more',\n",
       "  'competent',\n",
       "  'hands',\n",
       "  'turns',\n",
       "  'out',\n",
       "  'to',\n",
       "  'be',\n",
       "  'confusing',\n",
       "  'ill',\n",
       "  'considered',\n",
       "  'mess',\n",
       "  'under',\n",
       "  'the',\n",
       "  'sloppy',\n",
       "  'direction',\n",
       "  'of',\n",
       "  'john',\n",
       "  'shiban',\n",
       "  'who',\n",
       "  'also',\n",
       "  'wrote',\n",
       "  'the',\n",
       "  'screenplay',\n",
       "  'there',\n",
       "  'is',\n",
       "  'good',\n",
       "  'deal',\n",
       "  'of',\n",
       "  'juicy',\n",
       "  'violence',\n",
       "  'brief',\n",
       "  'smattering',\n",
       "  'of',\n",
       "  'nudity',\n",
       "  'and',\n",
       "  'confident',\n",
       "  'performances',\n",
       "  'from',\n",
       "  'the',\n",
       "  'cast',\n",
       "  'but',\n",
       "  'the',\n",
       "  'silly',\n",
       "  'script',\n",
       "  'leaves',\n",
       "  'the',\n",
       "  'viewer',\n",
       "  'with',\n",
       "  'so',\n",
       "  'many',\n",
       "  'unanswered',\n",
       "  'questions',\n",
       "  'one',\n",
       "  'cannot',\n",
       "  'help',\n",
       "  'but',\n",
       "  'feel',\n",
       "  'disappointed',\n",
       "  'on',\n",
       "  'the',\n",
       "  'surface',\n",
       "  'the',\n",
       "  'film',\n",
       "  'plays',\n",
       "  'out',\n",
       "  'like',\n",
       "  'standard',\n",
       "  'cliché',\n",
       "  'ridden',\n",
       "  'killer',\n",
       "  'on',\n",
       "  'the',\n",
       "  'loose',\n",
       "  'movie',\n",
       "  'but',\n",
       "  'shiban',\n",
       "  'an',\n",
       "  'ex',\n",
       "  'writer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'files',\n",
       "  'throws',\n",
       "  'in',\n",
       "  'some',\n",
       "  'subtle',\n",
       "  'supernatural',\n",
       "  'elements',\n",
       "  'which',\n",
       "  'suggest',\n",
       "  'that',\n",
       "  'his',\n",
       "  'aim',\n",
       "  'was',\n",
       "  'something',\n",
       "  'else',\n",
       "  'entirely',\n",
       "  'ghost',\n",
       "  'story',\n",
       "  'with',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'stop',\n",
       "  'acting',\n",
       "  'as',\n",
       "  'home',\n",
       "  'to',\n",
       "  'vengeful',\n",
       "  'spectre',\n",
       "  'out',\n",
       "  'to',\n",
       "  'punish',\n",
       "  'sinners',\n",
       "  'by',\n",
       "  'reading',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  'film',\n",
       "  'checking',\n",
       "  'out',\n",
       "  'viewers',\n",
       "  'theories',\n",
       "  'here',\n",
       "  'on',\n",
       "  'imdb',\n",
       "  'and',\n",
       "  'watching',\n",
       "  'the',\n",
       "  'extras',\n",
       "  'on',\n",
       "  'the',\n",
       "  'dvd',\n",
       "  'certain',\n",
       "  'plot',\n",
       "  'elements',\n",
       "  'begin',\n",
       "  'to',\n",
       "  'make',\n",
       "  'little',\n",
       "  'more',\n",
       "  'sense',\n",
       "  'although',\n",
       "  'even',\n",
       "  'with',\n",
       "  'the',\n",
       "  'advantage',\n",
       "  'of',\n",
       "  'extra',\n",
       "  'information',\n",
       "  'there',\n",
       "  'are',\n",
       "  'still',\n",
       "  'many',\n",
       "  'questions',\n",
       "  'left',\n",
       "  'unanswered',\n",
       "  'in',\n",
       "  'my',\n",
       "  'opinion',\n",
       "  'any',\n",
       "  'film',\n",
       "  'that',\n",
       "  'requires',\n",
       "  'this',\n",
       "  'much',\n",
       "  'investigation',\n",
       "  'to',\n",
       "  'make',\n",
       "  'itself',\n",
       "  'only',\n",
       "  'partially',\n",
       "  'understood',\n",
       "  'is',\n",
       "  'not',\n",
       "  'particularly',\n",
       "  'good',\n",
       "  'one'],\n",
       " ['ve',\n",
       "  'never',\n",
       "  'made',\n",
       "  'one',\n",
       "  'of',\n",
       "  'these',\n",
       "  'before',\n",
       "  'but',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'literally',\n",
       "  'so',\n",
       "  'bad',\n",
       "  'had',\n",
       "  'to',\n",
       "  'say',\n",
       "  'something',\n",
       "  'about',\n",
       "  'it',\n",
       "  'all',\n",
       "  'for',\n",
       "  'independent',\n",
       "  'film',\n",
       "  'making',\n",
       "  'as',\n",
       "  'the',\n",
       "  'past',\n",
       "  'year',\n",
       "  'has',\n",
       "  'seen',\n",
       "  'of',\n",
       "  'the',\n",
       "  'worst',\n",
       "  'in',\n",
       "  'my',\n",
       "  'opinion',\n",
       "  'of',\n",
       "  'hollywood',\n",
       "  'showings',\n",
       "  'the',\n",
       "  'mainstream',\n",
       "  'just',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'have',\n",
       "  'lost',\n",
       "  'touch',\n",
       "  'with',\n",
       "  'what',\n",
       "  'making',\n",
       "  'good',\n",
       "  'films',\n",
       "  'is',\n",
       "  'all',\n",
       "  'about',\n",
       "  'that',\n",
       "  'being',\n",
       "  'said',\n",
       "  'movies',\n",
       "  'like',\n",
       "  'this',\n",
       "  'really',\n",
       "  'give',\n",
       "  'independent',\n",
       "  'film',\n",
       "  'bad',\n",
       "  'reputation',\n",
       "  'the',\n",
       "  'characters',\n",
       "  'are',\n",
       "  'boring',\n",
       "  'and',\n",
       "  'too',\n",
       "  'stupid',\n",
       "  'to',\n",
       "  'empathize',\n",
       "  'with',\n",
       "  'the',\n",
       "  'direction',\n",
       "  'is',\n",
       "  'horrible',\n",
       "  'the',\n",
       "  'plotting',\n",
       "  'is',\n",
       "  'horrible',\n",
       "  'the',\n",
       "  'plot',\n",
       "  'itself',\n",
       "  'is',\n",
       "  'horrible',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'far',\n",
       "  'away',\n",
       "  'only',\n",
       "  'one',\n",
       "  'brief',\n",
       "  'scene',\n",
       "  'featuring',\n",
       "  'female',\n",
       "  'nude',\n",
       "  'breasts',\n",
       "  'and',\n",
       "  'even',\n",
       "  'that',\n",
       "  'wasn',\n",
       "  'worth',\n",
       "  'second',\n",
       "  'look',\n",
       "  'the',\n",
       "  'scariest',\n",
       "  'thing',\n",
       "  'about',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'of',\n",
       "  'ever',\n",
       "  'having',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'it',\n",
       "  'again',\n",
       "  'gave',\n",
       "  'it',\n",
       "  'and',\n",
       "  'not',\n",
       "  'simply',\n",
       "  'because',\n",
       "  'the',\n",
       "  'actors',\n",
       "  'were',\n",
       "  'visible',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sound',\n",
       "  'was',\n",
       "  'audible',\n",
       "  'it',\n",
       "  'earns',\n",
       "  'one',\n",
       "  'point',\n",
       "  'for',\n",
       "  'each',\n",
       "  'of',\n",
       "  'those',\n",
       "  'traits'],\n",
       " ['if',\n",
       "  'you',\n",
       "  'just',\n",
       "  'want',\n",
       "  'gore',\n",
       "  'and',\n",
       "  'nothing',\n",
       "  'but',\n",
       "  'gore',\n",
       "  'and',\n",
       "  'torture',\n",
       "  'you',\n",
       "  've',\n",
       "  'come',\n",
       "  'to',\n",
       "  'the',\n",
       "  'right',\n",
       "  'movie',\n",
       "  'if',\n",
       "  'you',\n",
       "  'want',\n",
       "  'at',\n",
       "  'least',\n",
       "  'sliver',\n",
       "  'of',\n",
       "  'good',\n",
       "  'acting',\n",
       "  'logic',\n",
       "  'story',\n",
       "  'consistencies',\n",
       "  'or',\n",
       "  'even',\n",
       "  'good',\n",
       "  'guy',\n",
       "  'ending',\n",
       "  'go',\n",
       "  'elsewhere',\n",
       "  'couldn',\n",
       "  'help',\n",
       "  'but',\n",
       "  'to',\n",
       "  'think',\n",
       "  'to',\n",
       "  'myself',\n",
       "  'jeeeez',\n",
       "  'are',\n",
       "  'those',\n",
       "  'people',\n",
       "  'mentally',\n",
       "  'challenged',\n",
       "  'example',\n",
       "  'after',\n",
       "  'being',\n",
       "  'chased',\n",
       "  'around',\n",
       "  'and',\n",
       "  'seeing',\n",
       "  'other',\n",
       "  'people',\n",
       "  'mutilated',\n",
       "  'the',\n",
       "  'main',\n",
       "  'actress',\n",
       "  'meets',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'and',\n",
       "  'spills',\n",
       "  'out',\n",
       "  'her',\n",
       "  'story',\n",
       "  'to',\n",
       "  'the',\n",
       "  'cop',\n",
       "  'with',\n",
       "  'tears',\n",
       "  'and',\n",
       "  'everything',\n",
       "  'and',\n",
       "  'told',\n",
       "  'him',\n",
       "  'about',\n",
       "  'the',\n",
       "  'psychopath',\n",
       "  'that',\n",
       "  'drives',\n",
       "  'in',\n",
       "  'yellow',\n",
       "  'truck',\n",
       "  'the',\n",
       "  'yellow',\n",
       "  'truck',\n",
       "  'pulls',\n",
       "  'up',\n",
       "  'and',\n",
       "  'the',\n",
       "  'officer',\n",
       "  'just',\n",
       "  'walks',\n",
       "  'to',\n",
       "  'it',\n",
       "  'talks',\n",
       "  'to',\n",
       "  'the',\n",
       "  'guy',\n",
       "  'and',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'drives',\n",
       "  'off',\n",
       "  'without',\n",
       "  'any',\n",
       "  'trouble',\n",
       "  'the',\n",
       "  'actress',\n",
       "  'comes',\n",
       "  'out',\n",
       "  'and',\n",
       "  'says',\n",
       "  'why',\n",
       "  'didn',\n",
       "  'you',\n",
       "  'arrest',\n",
       "  'him',\n",
       "  'and',\n",
       "  'then',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'runs',\n",
       "  'over',\n",
       "  'the',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'after',\n",
       "  'being',\n",
       "  'rammed',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'stops',\n",
       "  'on',\n",
       "  'the',\n",
       "  'road',\n",
       "  'about',\n",
       "  'feet',\n",
       "  'way',\n",
       "  'just',\n",
       "  'standing',\n",
       "  'there',\n",
       "  'while',\n",
       "  'the',\n",
       "  'actress',\n",
       "  'tries',\n",
       "  'to',\n",
       "  'drag',\n",
       "  'the',\n",
       "  'cop',\n",
       "  'away',\n",
       "  'but',\n",
       "  'he',\n",
       "  'too',\n",
       "  'heavy',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'at',\n",
       "  'that',\n",
       "  'time',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'backs',\n",
       "  'up',\n",
       "  'and',\n",
       "  'runs',\n",
       "  'over',\n",
       "  'the',\n",
       "  'cops',\n",
       "  'leg',\n",
       "  'twice',\n",
       "  'the',\n",
       "  'truck',\n",
       "  'then',\n",
       "  'drives',\n",
       "  'off',\n",
       "  'why',\n",
       "  'didn',\n",
       "  'the',\n",
       "  'actress',\n",
       "  'get',\n",
       "  'the',\n",
       "  'gun',\n",
       "  'is',\n",
       "  'beyond',\n",
       "  'me',\n",
       "  'which',\n",
       "  'later',\n",
       "  'she',\n",
       "  'shoots',\n",
       "  'the',\n",
       "  'cop',\n",
       "  'in',\n",
       "  'the',\n",
       "  'head',\n",
       "  'twice',\n",
       "  'because',\n",
       "  'the',\n",
       "  'psychopath',\n",
       "  'was',\n",
       "  'about',\n",
       "  'to',\n",
       "  'burn',\n",
       "  'him',\n",
       "  'alive',\n",
       "  'once',\n",
       "  'through',\n",
       "  'the',\n",
       "  'mouth',\n",
       "  'which',\n",
       "  'didn',\n",
       "  'kill',\n",
       "  'him',\n",
       "  'duuuumb',\n",
       "  'and',\n",
       "  'twice',\n",
       "  'to',\n",
       "  'finish',\n",
       "  'the',\n",
       "  'job',\n",
       "  'roll',\n",
       "  'eyes',\n",
       "  'right',\n",
       "  'after',\n",
       "  'that',\n",
       "  'she',\n",
       "  'turns',\n",
       "  'away',\n",
       "  'to',\n",
       "  'escape',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  'which',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'explode',\n",
       "  'and',\n",
       "  'when',\n",
       "  'she',\n",
       "  'climbs',\n",
       "  'near',\n",
       "  'the',\n",
       "  'roof',\n",
       "  'she',\n",
       "  'turns',\n",
       "  'around',\n",
       "  'and',\n",
       "  'the',\n",
       "  'cop',\n",
       "  'isn',\n",
       "  'there',\n",
       "  'anymore',\n",
       "  'ok',\n",
       "  'another',\n",
       "  'example',\n",
       "  'the',\n",
       "  'main',\n",
       "  'actress',\n",
       "  'meets',\n",
       "  'trapped',\n",
       "  'woman',\n",
       "  'in',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  'she',\n",
       "  'spits',\n",
       "  'out',\n",
       "  'like',\n",
       "  'gallon',\n",
       "  'of',\n",
       "  'blood',\n",
       "  'on',\n",
       "  'the',\n",
       "  'floor',\n",
       "  'covering',\n",
       "  'about',\n",
       "  'of',\n",
       "  'the',\n",
       "  'room',\n",
       "  'probably',\n",
       "  'more',\n",
       "  'after',\n",
       "  'the',\n",
       "  'main',\n",
       "  'actress',\n",
       "  'goes',\n",
       "  'outside',\n",
       "  'to',\n",
       "  'grab',\n",
       "  'towel',\n",
       "  'she',\n",
       "  'comes',\n",
       "  'back',\n",
       "  'in',\n",
       "  'and',\n",
       "  'everything',\n",
       "  'is',\n",
       "  'gone',\n",
       "  'they',\n",
       "  'don',\n",
       "  'explain',\n",
       "  'why',\n",
       "  'everyone',\n",
       "  'keeps',\n",
       "  'disappearing',\n",
       "  'either',\n",
       "  'dumb',\n",
       "  'dumb',\n",
       "  'dumb',\n",
       "  'like',\n",
       "  'horror',\n",
       "  'thriller',\n",
       "  'gore',\n",
       "  'movies',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'was',\n",
       "  'just',\n",
       "  'way',\n",
       "  'too',\n",
       "  'dumb',\n",
       "  'lost',\n",
       "  'brain',\n",
       "  'cells',\n",
       "  'watching',\n",
       "  'this',\n",
       "  'dribble',\n",
       "  'and',\n",
       "  'you',\n",
       "  'shouldn',\n",
       "  'too'],\n",
       " ['we',\n",
       "  'have',\n",
       "  'given',\n",
       "  'this',\n",
       "  'film',\n",
       "  'one',\n",
       "  'star',\n",
       "  'for',\n",
       "  'awful',\n",
       "  'however',\n",
       "  'it',\n",
       "  'really',\n",
       "  'depends',\n",
       "  'on',\n",
       "  'how',\n",
       "  'you',\n",
       "  'look',\n",
       "  'at',\n",
       "  'it',\n",
       "  'we',\n",
       "  'are',\n",
       "  'currently',\n",
       "  'watching',\n",
       "  'this',\n",
       "  'on',\n",
       "  'channel',\n",
       "  'at',\n",
       "  'am',\n",
       "  'and',\n",
       "  'we',\n",
       "  'haven',\n",
       "  'stopped',\n",
       "  'laughing',\n",
       "  'so',\n",
       "  'perhaps',\n",
       "  'we',\n",
       "  'could',\n",
       "  'give',\n",
       "  'it',\n",
       "  'just',\n",
       "  'for',\n",
       "  'the',\n",
       "  'entertainment',\n",
       "  'value',\n",
       "  'right',\n",
       "  'from',\n",
       "  'the',\n",
       "  'outset',\n",
       "  'with',\n",
       "  'the',\n",
       "  'manic',\n",
       "  'thoughts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'stalker',\n",
       "  'being',\n",
       "  'relayed',\n",
       "  'in',\n",
       "  'comic',\n",
       "  'fashion',\n",
       "  'we',\n",
       "  'were',\n",
       "  'laughing',\n",
       "  'is',\n",
       "  'it',\n",
       "  'me',\n",
       "  'or',\n",
       "  'does',\n",
       "  'that',\n",
       "  'chick',\n",
       "  'from',\n",
       "  'knott',\n",
       "  'landing',\n",
       "  'look',\n",
       "  'like',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'characters',\n",
       "  'from',\n",
       "  'the',\n",
       "  'dark',\n",
       "  'crystal',\n",
       "  'not',\n",
       "  'going',\n",
       "  'to',\n",
       "  'spoil',\n",
       "  'it',\n",
       "  'for',\n",
       "  'you',\n",
       "  'however',\n",
       "  'you',\n",
       "  'would',\n",
       "  'have',\n",
       "  'to',\n",
       "  'be',\n",
       "  'pretty',\n",
       "  'stupid',\n",
       "  'not',\n",
       "  'to',\n",
       "  'see',\n",
       "  'it',\n",
       "  'coming',\n",
       "  'don',\n",
       "  'think',\n",
       "  'its',\n",
       "  'clever',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'double',\n",
       "  'bluffs',\n",
       "  'this',\n",
       "  'has',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  'worst',\n",
       "  'best',\n",
       "  'film',\n",
       "  'we',\n",
       "  'have',\n",
       "  'ever',\n",
       "  'seen',\n",
       "  'if',\n",
       "  'we',\n",
       "  'been',\n",
       "  'playing',\n",
       "  'the',\n",
       "  'spot',\n",
       "  'the',\n",
       "  'cliché',\n",
       "  'drinking',\n",
       "  'game',\n",
       "  'then',\n",
       "  'we',\n",
       "  'be',\n",
       "  'wasted',\n",
       "  'by',\n",
       "  'now'],\n",
       " ['every',\n",
       "  'movie',\n",
       "  'have',\n",
       "  'ppv',\n",
       "  'because',\n",
       "  'leonard',\n",
       "  'maltin',\n",
       "  'praised',\n",
       "  'it',\n",
       "  'to',\n",
       "  'the',\n",
       "  'skies',\n",
       "  'has',\n",
       "  'blown',\n",
       "  'chunks',\n",
       "  'every',\n",
       "  'single',\n",
       "  'one',\n",
       "  'when',\n",
       "  'will',\n",
       "  'ever',\n",
       "  'learn',\n",
       "  'evie',\n",
       "  'is',\n",
       "  'raving',\n",
       "  'old',\n",
       "  'bag',\n",
       "  'who',\n",
       "  'thinks',\n",
       "  'nothing',\n",
       "  'of',\n",
       "  'saying',\n",
       "  'she',\n",
       "  'dying',\n",
       "  'of',\n",
       "  'breast',\n",
       "  'cancer',\n",
       "  'to',\n",
       "  'get',\n",
       "  'her',\n",
       "  'way',\n",
       "  'laura',\n",
       "  'is',\n",
       "  'an',\n",
       "  'insufferable',\n",
       "  'medusa',\n",
       "  'filled',\n",
       "  'with',\n",
       "  'the',\n",
       "  'holy',\n",
       "  'spirit',\n",
       "  'and',\n",
       "  'her',\n",
       "  'hubby',\n",
       "  'protégé',\n",
       "  'caught',\n",
       "  'between',\n",
       "  'these',\n",
       "  'harpies',\n",
       "  'is',\n",
       "  'medusa',\n",
       "  'dumb',\n",
       "  'as',\n",
       "  'rock',\n",
       "  'boy',\n",
       "  'who',\n",
       "  'has',\n",
       "  'been',\n",
       "  'pressed',\n",
       "  'into',\n",
       "  'weed',\n",
       "  'pulling',\n",
       "  'servitude',\n",
       "  'by',\n",
       "  'the',\n",
       "  'old',\n",
       "  'bag',\n",
       "  'as',\n",
       "  'said',\n",
       "  'when',\n",
       "  'will',\n",
       "  'ever',\n",
       "  'learn',\n",
       "  'was',\n",
       "  'temporarily',\n",
       "  'lifted',\n",
       "  'out',\n",
       "  'of',\n",
       "  'my',\n",
       "  'malaise',\n",
       "  'when',\n",
       "  'the',\n",
       "  'old',\n",
       "  'bag',\n",
       "  'stuck',\n",
       "  'her',\n",
       "  'head',\n",
       "  'in',\n",
       "  'sink',\n",
       "  'but',\n",
       "  'unfortunately',\n",
       "  'she',\n",
       "  'did',\n",
       "  'not',\n",
       "  'die',\n",
       "  'was',\n",
       "  'temporarily',\n",
       "  'lifted',\n",
       "  'out',\n",
       "  'of',\n",
       "  'my',\n",
       "  'malaise',\n",
       "  'again',\n",
       "  'when',\n",
       "  'medusa',\n",
       "  'got',\n",
       "  'mowed',\n",
       "  'down',\n",
       "  'but',\n",
       "  'unfortunately',\n",
       "  'she',\n",
       "  'did',\n",
       "  'not',\n",
       "  'die',\n",
       "  'it',\n",
       "  'should',\n",
       "  'be',\n",
       "  'capital',\n",
       "  'offense',\n",
       "  'to',\n",
       "  'torture',\n",
       "  'audiences',\n",
       "  'like',\n",
       "  'this',\n",
       "  'without',\n",
       "  'harry',\n",
       "  'potter',\n",
       "  'to',\n",
       "  'kick',\n",
       "  'him',\n",
       "  'around',\n",
       "  'rupert',\n",
       "  'grint',\n",
       "  'is',\n",
       "  'just',\n",
       "  'pair',\n",
       "  'of',\n",
       "  'big',\n",
       "  'blue',\n",
       "  'eyes',\n",
       "  'that',\n",
       "  'practically',\n",
       "  'bulge',\n",
       "  'out',\n",
       "  'of',\n",
       "  'its',\n",
       "  'sockets',\n",
       "  'julie',\n",
       "  'walters',\n",
       "  'scenery',\n",
       "  'chewing',\n",
       "  'especially',\n",
       "  'the',\n",
       "  'scene',\n",
       "  'when',\n",
       "  'she',\n",
       "  'plays',\n",
       "  'god',\n",
       "  'is',\n",
       "  'even',\n",
       "  'more',\n",
       "  'shameless',\n",
       "  'than',\n",
       "  'her',\n",
       "  'character',\n",
       "  'at',\n",
       "  'least',\n",
       "  'this',\n",
       "  'harold',\n",
       "  'bangs',\n",
       "  'some',\n",
       "  'bimbo',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'maude',\n",
       "  'for',\n",
       "  'that',\n",
       "  'am',\n",
       "  'truly',\n",
       "  'grateful',\n",
       "  'and',\n",
       "  'if',\n",
       "  'you',\n",
       "  're',\n",
       "  'reading',\n",
       "  'this',\n",
       "  'mr',\n",
       "  'maltin',\n",
       "  'you',\n",
       "  'owe',\n",
       "  'me'],\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "d; awaiting finish of 2 more threads\n",
      "2020-11-15 13:52:34,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:52:34,952 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:52:34,952 : INFO : EPOCH - 4 : training on 10974519 raw words (8411376 effective words) took 6.3s, 1335056 effective words/s\n",
      "2020-11-15 13:52:35,957 : INFO : EPOCH 5 - PROGRESS: at 15.50% examples, 1270102 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:36,959 : INFO : EPOCH 5 - PROGRESS: at 31.30% examples, 1296634 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:52:37,964 : INFO : EPOCH 5 - PROGRESS: at 47.53% examples, 1313817 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:38,964 : INFO : EPOCH 5 - PROGRESS: at 63.68% examples, 1321650 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:52:39,974 : INFO : EPOCH 5 - PROGRESS: at 79.52% examples, 1324460 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:52:40,985 : INFO : EPOCH 5 - PROGRESS: at 95.14% examples, 1325779 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:41,239 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:52:41,244 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:52:41,247 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:52:41,249 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:52:41,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:52:41,257 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:52:41,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:52:41,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:52:41,274 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:52:41,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:52:41,280 : INFO : EPOCH - 5 : training on 10974519 raw words (8412884 effective words) took 6.3s, 1330445 effective words/s\n",
      "2020-11-15 13:52:41,281 : INFO : training on a 54872595 raw words (42062886 effective words) took 31.8s, 1322235 effective words/s\n",
      "2020-11-15 13:52:41,282 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-11-15 13:52:41,282 : INFO : training model with 10 workers on 62093 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-11-15 13:52:42,296 : INFO : EPOCH 1 - PROGRESS: at 15.95% examples, 1296372 words/s, in_qsize 20, out_qsize 0\n",
      "2020-11-15 13:52:43,308 : INFO : EPOCH 1 - PROGRESS: at 31.98% examples, 1313997 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:44,309 : INFO : EPOCH 1 - PROGRESS: at 48.39% examples, 1329672 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:45,313 : INFO : EPOCH 1 - PROGRESS: at 64.54% examples, 1332403 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:46,320 : INFO : EPOCH 1 - PROGRESS: at 79.89% examples, 1328066 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:52:47,321 : INFO : EPOCH 1 - PROGRESS: at 95.15% examples, 1324353 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:52:47,594 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:52:47,601 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:52:47,604 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:52:47,608 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:52:47,614 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:52:47,620 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:52:47,621 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:52:47,630 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:52:47,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:52:47,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:52:47,634 : INFO : EPOCH - 1 : training on 10974519 raw words (8412543 effective words) took 6.3s, 1325490 effective words/s\n",
      "2020-11-15 13:52:48,639 : INFO : EPOCH 2 - PROGRESS: at 15.77% examples, 1291775 words/s, in_qsize 20, out_qsize 0\n",
      "2020-11-15 13:52:49,643 : INFO : EPOCH 2 - PROGRESS: at 32.06% examples, 1327706 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:50,648 : INFO : EPOCH 2 - PROGRESS: at 48.00% examples, 1324309 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:51,650 : INFO : EPOCH 2 - PROGRESS: at 64.34% examples, 1333053 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:52,652 : INFO : EPOCH 2 - PROGRESS: at 80.00% examples, 1334291 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:52:53,652 : INFO : EPOCH 2 - PROGRESS: at 95.68% examples, 1336190 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:53,890 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:52:53,894 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:52:53,897 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:52:53,900 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:52:53,906 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:52:53,907 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:52:53,914 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:52:53,924 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:52:53,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:52:53,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:52:53,927 : INFO : EPOCH - 2 : training on 10974519 raw words (8413440 effective words) took 6.3s, 1337591 effective words/s\n",
      "2020-11-15 13:52:54,940 : INFO : EPOCH 3 - PROGRESS: at 16.12% examples, 1313617 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:55,942 : INFO : EPOCH 3 - PROGRESS: at 32.14% examples, 1329589 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:56,948 : INFO : EPOCH 3 - PROGRESS: at 48.20% examples, 1327633 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:57,956 : INFO : EPOCH 3 - PROGRESS: at 64.15% examples, 1325845 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:52:58,958 : INFO : EPOCH 3 - PROGRESS: at 79.76% examples, 1326775 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:52:59,968 : INFO : EPOCH 3 - PROGRESS: at 95.65% examples, 1331396 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:00,208 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:00,217 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:00,220 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:00,226 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:00,227 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:00,231 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:00,233 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:00,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:00,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:00,250 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:00,251 : INFO : EPOCH - 3 : training on 10974519 raw words (8412324 effective words) took 6.3s, 1331450 effective words/s\n",
      "2020-11-15 13:53:01,259 : INFO : EPOCH 4 - PROGRESS: at 15.93% examples, 1304228 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:02,260 : INFO : EPOCH 4 - PROGRESS: at 32.06% examples, 1328320 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:03,261 : INFO : EPOCH 4 - PROGRESS: at 48.39% examples, 1336903 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:04,270 : INFO : EPOCH 4 - PROGRESS: at 64.55% examples, 1336268 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:05,273 : INFO : EPOCH 4 - PROGRESS: at 80.34% examples, 1339429 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:06,286 : INFO : EPOCH 4 - PROGRESS: at 95.80% examples, 1334992 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:06,506 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:06,518 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:06,522 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:06,524 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:06,528 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:06,530 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:06,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:06,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:06,545 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:06,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:06,550 : INFO : EPOCH - 4 : training on 10974519 raw words (8413100 effective words) took 6.3s, 1336538 effective words/s\n",
      "2020-11-15 13:53:07,557 : INFO : EPOCH 5 - PROGRESS: at 15.59% examples, 1274623 words/s, in_qsize 19, out_qsize 1\n",
      "2020-11-15 13:53:08,570 : INFO : EPOCH 5 - PROGRESS: at 31.70% examples, 1306566 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:09,574 : INFO : EPOCH 5 - PROGRESS: at 47.84% examples, 1315502 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:10,583 : INFO : EPOCH 5 - PROGRESS: at 64.06% examples, 1321994 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:11,589 : INFO : EPOCH 5 - PROGRESS: at 79.97% examples, 1329084 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:12,593 : INFO : EPOCH 5 - PROGRESS: at 95.58% examples, 1329592 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:12,829 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:12,833 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:12,839 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:12,840 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:12,848 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:12,849 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:12,850 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:12,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:12,861 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:12,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:12,863 : INFO : EPOCH - 5 : training on 10974519 raw words (8414477 effective words) took 6.3s, 1333753 effective words/s\n",
      "2020-11-15 13:53:13,867 : INFO : EPOCH 6 - PROGRESS: at 16.01% examples, 1315055 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:14,868 : INFO : EPOCH 6 - PROGRESS: at 31.92% examples, 1326962 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:15,870 : INFO : EPOCH 6 - PROGRESS: at 48.08% examples, 1330517 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:16,872 : INFO : EPOCH 6 - PROGRESS: at 64.16% examples, 1331895 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:17,886 : INFO : EPOCH 6 - PROGRESS: at 79.84% examples, 1330077 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:18,901 : INFO : EPOCH 6 - PROGRESS: at 95.73% examples, 1332873 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:19,130 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:19,133 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:19,140 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:19,143 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:19,144 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:19,147 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:19,148 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:19,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:19,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:19,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:19,163 : INFO : EPOCH - 6 : training on 10974519 raw words (8412369 effective words) took 6.3s, 1335999 effective words/s\n",
      "2020-11-15 13:53:20,171 : INFO : EPOCH 7 - PROGRESS: at 16.10% examples, 1318997 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:21,174 : INFO : EPOCH 7 - PROGRESS: at 32.33% examples, 1338299 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:22,180 : INFO : EPOCH 7 - PROGRESS: at 48.48% examples, 1336491 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:23,190 : INFO : EPOCH 7 - PROGRESS: at 64.54% examples, 1333547 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:24,192 : INFO : EPOCH 7 - PROGRESS: at 80.27% examples, 1336068 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:25,207 : INFO : EPOCH 7 - PROGRESS: at 95.96% examples, 1335386 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:25,414 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:25,420 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:25,427 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:25,431 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:25,438 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:25,443 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:25,452 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:25,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:25,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:25,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:25,459 : INFO : EPOCH - 7 : training on 10974519 raw words (8412588 effective words) took 6.3s, 1336975 effective words/s\n",
      "2020-11-15 13:53:26,476 : INFO : EPOCH 8 - PROGRESS: at 15.93% examples, 1291316 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:27,483 : INFO : EPOCH 8 - PROGRESS: at 31.98% examples, 1315069 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:53:28,489 : INFO : EPOCH 8 - PROGRESS: at 48.27% examples, 1325235 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:53:29,491 : INFO : EPOCH 8 - PROGRESS: at 64.45% examples, 1330098 words/s, in_qsize 17, out_qsize 2\n",
      "2020-11-15 13:53:30,493 : INFO : EPOCH 8 - PROGRESS: at 80.17% examples, 1333163 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:31,496 : INFO : EPOCH 8 - PROGRESS: at 95.88% examples, 1335578 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:31,710 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:31,713 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:31,720 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:31,723 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:31,724 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:31,733 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:31,734 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:31,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:31,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:31,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:31,752 : INFO : EPOCH - 8 : training on 10974519 raw words (8411142 effective words) took 6.3s, 1337560 effective words/s\n",
      "2020-11-15 13:53:32,758 : INFO : EPOCH 9 - PROGRESS: at 15.95% examples, 1305776 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:33,758 : INFO : EPOCH 9 - PROGRESS: at 32.14% examples, 1334013 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:34,759 : INFO : EPOCH 9 - PROGRESS: at 48.29% examples, 1335595 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:35,769 : INFO : EPOCH 9 - PROGRESS: at 64.54% examples, 1336628 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:36,773 : INFO : EPOCH 9 - PROGRESS: at 80.27% examples, 1337899 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:37,779 : INFO : EPOCH 9 - PROGRESS: at 95.75% examples, 1335404 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:38,013 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:38,021 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:38,024 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:38,026 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:38,027 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:38,030 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:38,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:38,042 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:38,043 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:38,044 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:38,044 : INFO : EPOCH - 9 : training on 10974519 raw words (8412707 effective words) took 6.3s, 1337851 effective words/s\n",
      "2020-11-15 13:53:39,053 : INFO : EPOCH 10 - PROGRESS: at 15.86% examples, 1294467 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:40,055 : INFO : EPOCH 10 - PROGRESS: at 31.61% examples, 1308519 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:41,058 : INFO : EPOCH 10 - PROGRESS: at 48.00% examples, 1324703 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:42,060 : INFO : EPOCH 10 - PROGRESS: at 64.06% examples, 1327492 words/s, in_qsize 18, out_qsize 1\n",
      "2020-11-15 13:53:43,072 : INFO : EPOCH 10 - PROGRESS: at 79.93% examples, 1330006 words/s, in_qsize 20, out_qsize 3\n",
      "2020-11-15 13:53:44,074 : INFO : EPOCH 10 - PROGRESS: at 95.68% examples, 1333446 words/s, in_qsize 19, out_qsize 0\n",
      "2020-11-15 13:53:44,317 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-11-15 13:53:44,321 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-11-15 13:53:44,325 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-11-15 13:53:44,326 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-11-15 13:53:44,328 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-11-15 13:53:44,333 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-11-15 13:53:44,336 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-11-15 13:53:44,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-11-15 13:53:44,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-11-15 13:53:44,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-11-15 13:53:44,349 : INFO : EPOCH - 10 : training on 10974519 raw words (8411657 effective words) took 6.3s, 1334871 effective words/s\n",
      "2020-11-15 13:53:44,350 : INFO : training on a 109745190 raw words (84126347 effective words) took 63.1s, 1333920 effective words/s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(84126347, 109745190)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `boring`. All we need to do here is to call the `most_similar` function and provide the word `boring` as the positive example. This returns the top 10 similar words (nearest neighbors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-11-15 13:53:44,365 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('dull', 0.7930447459220886),\n",
       " ('tedious', 0.7073862552642822),\n",
       " ('pointless', 0.6814718246459961),\n",
       " ('uninteresting', 0.6390188336372375),\n",
       " ('predictable', 0.6207038760185242),\n",
       " ('uneventful', 0.6168208718299866),\n",
       " ('confusing', 0.6135261654853821),\n",
       " ('meaningless', 0.6114116907119751),\n",
       " ('repetitive', 0.6027559638023376),\n",
       " ('unoriginal', 0.5979882478713989)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "w1 = \"boring\"\n",
    "model.wv.most_similar(positive=[w1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('stubborn', 0.5525908470153809), ('rude', 0.5403105616569519), ('naive', 0.5330991744995117), ('withdrawn', 0.5108522772789001), ('sensitive', 0.5086380243301392), ('conceited', 0.5052778124809265)]\n"
     ]
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "df=model.wv.most_similar(positive=w1)\n",
    "print(df[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('spain', 0.7504943609237671), ('england', 0.7110885381698608), ('italy', 0.6984615921974182), ('germany', 0.6935436129570007), ('russia', 0.6747918725013733), ('europe', 0.6714396476745605)]\n"
     ]
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "df=model.wv.most_similar(positive=w1)\n",
    "print(df[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('amazed', 0.7389718294143677), ('surprised', 0.7292163372039795), ('appalled', 0.6932759284973145), ('stunned', 0.6799626350402832), ('astonished', 0.6732420921325684), ('disgusted', 0.6556116342544556)]\n"
     ]
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "df=model.wv.most_similar(positive=w1)\n",
    "print(df[0:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('meal', 0.5071399211883545),\n",
       " ('bath', 0.4899105429649353),\n",
       " ('lemonade', 0.48824217915534973),\n",
       " ('flowers', 0.46308645606040955),\n",
       " ('chocolate', 0.46211183071136475),\n",
       " ('joints', 0.455669105052948),\n",
       " ('snack', 0.44953450560569763),\n",
       " ('acid', 0.44833898544311523),\n",
       " ('puppies', 0.4460829198360443),\n",
       " ('vegetables', 0.4450179636478424)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"food\",'cake','fruit']\n",
    "w2 = ['spoon']\n",
    "model.wv.most_similar(positive=w1, negative=w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.71336734"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# similarity between synonyms\n",
    "w1= \"good\"\n",
    "w2= \"great\"\n",
    "\n",
    "model.wv.similarity(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.078864224"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# similarity between two opposite words\n",
    "w1= \"great\"\n",
    "w2= \"aweful\"\n",
    "\n",
    "model.wv.similarity(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.21182805"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "w1= \"food\"\n",
    "w2= \"car\"\n",
    "model.wv.similarity(w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `boring` is highly similar to `dull` but `great` is dissimilar to `aweful`. Antonyms is a special case, and we managed to model them well since we used a movie reviews dataset, which includes separate positive and negative reviews. Using a general dataset won't necessarily model these antonyms correctly, since generally antonyms have very similar distributions in text. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "france\n"
     ]
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "list_of_words = [\"cat\",\"dog\",\"france\"]\n",
    "\n",
    "pairs = []\n",
    "values = []\n",
    "for i in range(len(list_of_words)):\n",
    "    for j in range(len(list_of_words)):\n",
    "        if i<j:\n",
    "            values.append(model.wv.similarity(list_of_words[i],list_of_words[j]))\n",
    "            pairs.append([list_of_words[i], list_of_words[j]])\n",
    "\n",
    "index = values.index(max(values))\n",
    "\n",
    "for word in list_of_words:\n",
    "    if word not in pairs[index]:\n",
    "        print(word)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6440322 dog cat\n0.42674297 dog horse\n0.11138967 dog shower\n0.6440322 cat dog\n0.27482846 cat horse\n0.057583783 cat shower\n0.42674297 horse dog\n0.27482846 horse cat\n0.12003072 horse shower\n0.11138967 shower dog\n0.057583783 shower cat\n0.12003072 shower horse\nshower is the odd one out\n"
     ]
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "list_of_words = [\"dog\",\"cat\",\"horse\",\"shower\"]\n",
    "\n",
    "for w1 in list_of_words:\n",
    "    for w2 in list_of_words:\n",
    "        if w1 != w2:\n",
    "            print(model.wv.similarity(w1,w2), w1,w2)\n",
    "\n",
    "print(\"shower is the odd one out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}